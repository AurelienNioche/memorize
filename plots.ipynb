{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "import multiprocessing as MP\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from scipy.stats import spearmanr, kendalltau, mannwhitneyu\n",
    "import datetime\n",
    "import matplotlib as mpl\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"paper\", font_scale=2.5, rc={\"lines.linewidth\": 3,\n",
    "                                           'lines.markersize': 10,\n",
    "                                           'legend.fontsize': 24})\n",
    "sns.set_palette(\"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions and constants to run the experiments are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i spaced_rep_code.py\n",
    "%run -i plot_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data files should be kept at these paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = \"./data/duolingo_reduced.csv\"\n",
    "dict_data = \"./data/duo_dict.dill\"\n",
    "\n",
    "model_weights_file = \"power.duolingo.weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 s, sys: 1.31 s, total: 15.8 s\n",
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_duo = pd.read_csv(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_duo['lexeme_comp'] = df_duo['learning_language']+\":\"+df_duo['lexeme_string']\n",
    "convert = df_duo[['lexeme_comp','lexeme_id']].set_index('lexeme_comp').to_dict()['lexeme_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44 ms, sys: 36.6 ms, total: 80.6 ms\n",
      "Wall time: 80.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_duo = pd.read_csv(open(model_weights_file, 'rb'),\n",
    "                          sep=\"\\t\",\n",
    "                          names=['label','value'],\n",
    "                          header=None)\n",
    "\n",
    "results_duo = results_duo.set_index(\"label\")\n",
    "\n",
    "B = results_duo.loc[\"B\"]\n",
    "start = 5\n",
    "\n",
    "duo_lexeme_difficulty = 2**(-(results_duo[start:]+results_duo.loc['bias']))\n",
    "new_index = []\n",
    "for ind in duo_lexeme_difficulty.index:\n",
    "    new_index.append(convert[ind])\n",
    "    \n",
    "duo_lexeme_difficulty['new_index'] = new_index\n",
    "duo_lexeme_difficulty.set_index('new_index',inplace=True)\n",
    "\n",
    "duo_map_lexeme = dict([(l_id,ind) for ind, l_id in enumerate(duo_lexeme_difficulty.index)])\n",
    "\n",
    "duo_lexeme_difficulty = duo_lexeme_difficulty['value'].tolist()\n",
    "duo_alpha = (-2**(-results_duo.loc['right'])+1).iloc[0]\n",
    "duo_beta = (2**(-results_duo.loc['wrong'])-1).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 1min 14s, total: 2min 15s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "duo_dict = dill.load(open(dict_data, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107867"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(duo_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.2 s, sys: 1.92 s, total: 18.2 s\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "duo_pairs = get_unique_user_lexeme(duo_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 2min 2s, total: 3min 20s\n",
      "Wall time: 4min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "initial_cond = df_duo[[\"lexeme_id\",\"user_id\",\"timestamp\"]].groupby([\"lexeme_id\",\"user_id\"]).min()[\"timestamp\"]\n",
    "initial_cond = initial_cond.to_dict()\n",
    "df_duo_index_set = df_duo.copy().set_index([\"lexeme_id\",\"user_id\",\"timestamp\"])\n",
    "df_duo_index_set.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "total_correct = []\n",
    "total_seen = []\n",
    "for ind, (u_id,l_id) in enumerate(duo_pairs):\n",
    "    for item in duo_dict[u_id][l_id]:\n",
    "        time = initial_cond[(l_id,u_id)]\n",
    "        correct = df_duo_index_set[\"history_correct\"].loc[(l_id,u_id,time)].tolist()[0]\n",
    "        seen = df_duo_index_set[\"history_seen\"].loc[(l_id,u_id,time)].tolist()[0]\n",
    "        total_correct.append(correct)\n",
    "        total_seen.append(seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "right = results_duo.loc['right'][0]\n",
    "wrong = results_duo.loc['wrong'][0]\n",
    "temp_duo_map_lexeme = {}\n",
    "for ind, (u_id,l_id) in enumerate(duo_pairs):\n",
    "    if ind % 1000 == 0:\n",
    "        print(datetime.datetime.now().isoformat(), ind, '/', len(duo_pairs))\n",
    "    for item in duo_dict[u_id][l_id]:\n",
    "        time = initial_cond[(l_id,u_id)]\n",
    "        correct = df_duo_index_set[\"history_correct\"].loc[(l_id,u_id,time)].tolist()[0]\n",
    "        seen = df_duo_index_set[\"history_seen\"].loc[(l_id,u_id,time)].tolist()[0]\n",
    "        #print(correct, seen)\n",
    "        temp = None\n",
    "        if l_id not in temp_duo_map_lexeme:\n",
    "            temp_duo_map_lexeme[l_id] = duo_lexeme_difficulty[duo_map_lexeme[l_id]]\n",
    "        temp = temp_duo_map_lexeme[l_id]\n",
    "        item['n_0'] =temp*\\\n",
    "                        2**(-(right*correct+\\\n",
    "                              wrong*(seen-correct)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "training_pairs = get_training_pairs(duo_dict, duo_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:.2f}% of sequences can be used for training/testing.'\n",
    "      .format(len(training_pairs) / len(duo_pairs) * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "duo_stats_99 = calc_user_LL_dict(duo_dict, duo_alpha, duo_beta, duo_lexeme_difficulty, duo_map_lexeme, \n",
    "                            success_prob=0.99, pairs=duo_pairs, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('error')\n",
    "    duo_stats_training = calc_user_LL_dict(\n",
    "        duo_dict, duo_alpha, duo_beta, duo_lexeme_difficulty, duo_map_lexeme, \n",
    "        success_prob=0.99, training=True, pairs=training_pairs, verbose=False,\n",
    "        n_procs=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "threshold_LL = calc_LL_dict_threshold(duo_dict, alpha=duo_alpha, beta=duo_beta, pairs=duo_pairs, \n",
    "                                      lexeme_difficulty=duo_lexeme_difficulty, map_lexeme=duo_map_lexeme,\n",
    "                                      success_prob=0.99, verbose=False)\n",
    "merge_with_thres_LL(duo_stats_99, threshold_LL, pairs=duo_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "threshold_LL_training = calc_user_LL_dict_threshold(\n",
    "    duo_dict, alpha=duo_alpha, beta=duo_beta, pairs=training_pairs, \n",
    "    lexeme_difficulty=duo_lexeme_difficulty, map_lexeme=duo_map_lexeme,\n",
    "    success_prob=0.99, verbose=False, training=True)\n",
    "\n",
    "merge_with_thres_LL(duo_stats_training, threshold_LL_training, pairs=training_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "duo_durations = get_all_durations(duo_dict, duo_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Different sequences be chosen for different T\n",
    "# The paper contains plots correspondnig to T \\in {3, 5, 7}\n",
    "middle_dur_pairs = filter_by_duration(\n",
    "    durations_dict=duo_durations, \n",
    "    pairs=training_pairs, \n",
    "    T=3, alpha=0.1,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "duo_forgetting_rate = calc_empirical_forgetting_rate(duo_dict, pairs=duo_pairs, no_norm=False)\n",
    "base = calc_empirical_forgetting_rate(duo_dict, pairs=duo_pairs, return_base=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "perf = duo_forgetting_rate\n",
    "with_exact_reps = True\n",
    "quantile = 0.25\n",
    "with_training = True\n",
    "\n",
    "def top_k_reps_worker(reps):\n",
    "    max_reps = None if not with_exact_reps else reps + 1\n",
    "    stats_dict = duo_stats_99 if not with_training else duo_stats_training\n",
    "    # pairs = duo_pairs if not with_training else training_pairs\n",
    "    pairs = duo_pairs if not with_training else middle_dur_pairs\n",
    "    return reps, calc_top_k_perf(stats_dict, perf, pairs=pairs, quantile=quantile,\n",
    "                                 min_reps=reps, max_reps=max_reps, with_overall=True,\n",
    "                                 only_finite=False, with_threshold=True)\n",
    "\n",
    "reps_range = np.arange(1 if not with_training else 2, 8)\n",
    "\n",
    "# For performance\n",
    "with MP.Pool(9) as pool:\n",
    "    top_k_99_reps = pool.map(top_k_reps_worker, reps_range)\n",
    "\n",
    "# For debugging\n",
    "# top_k_99_reps = []\n",
    "# for i in reps_range:\n",
    "#    top_k_99_reps.append(top_k_reps_worker(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {}\n",
    "for i in range(len(top_k_99_reps)): \n",
    "    mem_thresh = mannwhitneyu(top_k_99_reps[i][1]['perf_top_threshold'],\n",
    "             top_k_99_reps[i][1]['perf_top_mem'])\n",
    "    mem_unif = mannwhitneyu(\n",
    "             top_k_99_reps[i][1]['perf_top_mem'],top_k_99_reps[i][1]['perf_top_unif'])\n",
    "    stats[top_k_99_reps[i][0]]=(mem_thresh, mem_unif)\n",
    "    print(top_k_99_reps[i][0],\"MEM vs. Threshold\", stats[top_k_99_reps[i][0]][0])\n",
    "    print(top_k_99_reps[i][0],\"MEM vs. Uniform\",  stats[top_k_99_reps[i][0]][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T=7.0 alpha=0.1\n",
    "# latexify(fig_width=2,largeFonts=False,columns=2,font_scale=1.0)\n",
    "latexify(fig_width=3.4, largeFonts=True)\n",
    "\n",
    "plot_perf_by_reps_boxed(top_k_99_reps, with_threshold=True, std=False, \n",
    "                        max_rev=7, median=True, stats=stats)\n",
    "\n",
    "format_axes(plt.gca())\n",
    "plt.ylabel(\"$\\hat{n}$\")\n",
    "plt.xlabel(\"\\# reviews\")\n",
    "\n",
    "plt.ylim(0, 0.4)\n",
    "# plt.savefig(plot_path('empirical_p_recall_duo_new_boxed_split_T_3.pdf'), bbox_inches='tight',pad_inches=0)\n",
    "# plt.savefig(plot_path('empirical_p_recall_duo_new_boxed.pdf'), bbox_inches='tight',pad_inches=0)\n",
    "#plt.xlim(0,6*2+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_dur_pairs = filter_by_duration(\n",
    "    durations_dict=duo_durations, \n",
    "    pairs=training_pairs, \n",
    "    T=8, alpha=0.4,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "users = list(map(lambda f:f[0], middle_dur_pairs))\n",
    "\n",
    "from collections import OrderedDict\n",
    "users_ = {}\n",
    "for u in users:\n",
    "    if u in users_:\n",
    "        users_[u] += 1\n",
    "    else:\n",
    "        users_[u] = 0\n",
    "users_ = OrderedDict(sorted(users_.items(), key=lambda x: -x[1]))\n",
    "\n",
    "users_ = OrderedDict(filter(lambda x: x[1]>70, users_.items()))\n",
    "print('Number of users = ', len(users_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = []\n",
    "for ind, u in enumerate(users_):\n",
    "    if ind % 100 == 0:\n",
    "        print(\"Completed = \", ind / len(users_))\n",
    "    \n",
    "    middle_dur_pairs_users = filter_by_users(middle_dur_pairs, [u], False)    \n",
    "    perf = duo_forgetting_rate\n",
    "    with_exact_reps = True\n",
    "    with_training = True\n",
    "    threshold = 1.0\n",
    "    \n",
    "    def top_k_reps_worker(reps):\n",
    "        max_reps = None if not with_exact_reps else reps + 1\n",
    "        stats_dict = duo_stats_99 if not with_training else duo_stats_training\n",
    "        # pairs = duo_pairs if not with_training else training_pairs\n",
    "        pairs = middle_dur_pairs_users\n",
    "        return reps, calc_top_memorize(stats_dict, perf, pairs=pairs, \n",
    "                                     min_reps=reps, max_reps=max_reps, with_overall=True,\n",
    "                                     only_finite=True, with_threshold=True)\n",
    "\n",
    "    reps_range = np.arange(1 if not with_training else 2, 10)\n",
    "\n",
    "    # For performance\n",
    "    #with MP.Pool(9) as pool:\n",
    "    #    top_k_99_reps = pool.map(top_k_reps_worker, reps_range)\n",
    "    #\n",
    "    # For debugging\n",
    "    top_k_99_reps = []\n",
    "    for i in reps_range:\n",
    "        reps, (corr_mem, corr_uniform, corr_thresh) = top_k_reps_worker(i)\n",
    "        if corr_mem[1] < threshold:\n",
    "            results.append((u, reps, corr_mem[0], \"Memorize\" ))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, c2, c3 = sns.color_palette(\"Set2\",n_colors=3)\n",
    "df = pd.DataFrame(results, columns=[\"user\", \"Repetitions\", \n",
    "                                    \"Pearson Correlation\\nCoefficient\", \"Policy\"])\n",
    "#df.plot(\"reps\",\"correlation\",kind=\"bar\")\n",
    "df = df[df['Repetitions']<8]\n",
    "df = df[df[\"Policy\"]==\"Memorize\"]\n",
    "sns.catplot(\"Repetitions\", \"Pearson Correlation\\nCoefficient\",hue=\"Policy\", data=df, size=2.2,\n",
    "               aspect=2, legend=False, estimator=np.median, ci=68,capsize=.2,#,hue_order=[\"Memorize\", \"Threshold\",\"Uniform\"],\n",
    "               palette=sns.color_palette(\"Set2\",n_colors=3), linestyles=[\"\"])\n",
    "plt.hlines(0,-1,7, linestyle=\"--\")\n",
    "plt.show()\n",
    "#plt.legend(loc=9, ncol=3)\n",
    "#plt.savefig(plot_path('user_n_perf_all.pdf'), \n",
    "#            bbox_inches='tight',pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
